{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessedClaim():\n",
    "    \n",
    "    def createPosTags(self,paragraph,singleLine=0):\n",
    "        posTagContent1=[]\n",
    "        if(singleLine==0):\n",
    "            for line in paragraph:\n",
    "                line= line.split(' ',2)\n",
    "                sentenceID=line[1]\n",
    "                content=line[2]\n",
    "                posTagContent1.append(self.getChunks(content))\n",
    "        else:\n",
    "            content=paragraph\n",
    "            posTagContent1.append(self.getChunks(content))\n",
    "        return posTagContent1\n",
    "    \n",
    "    def getAntonyms(self,term):\n",
    "        synsets=wordnet.synsets(term)\n",
    "        for synset in synsets:\n",
    "            if(synset.pos()=='a'):\n",
    "                return synset.lemmas()[0].antonyms()[0].name()\n",
    "        return term\n",
    "\n",
    "    def getNegation(self,prev_term,term):\n",
    "        if prev_term==\"not\":\n",
    "            return term\n",
    "        else:\n",
    "            return \"not \"+term\n",
    "    \n",
    "    def getChunks(self,content):\n",
    "        chunks = []\n",
    "        contentToken = word_tokenize(content)\n",
    "        nc_pos = pos_tag(contentToken)\n",
    "\n",
    "        prevPosition = nc_pos[0][1]\n",
    "        entity = {\"pos\":prevPosition,\"chunk\":[]}\n",
    "        for c_node in nc_pos:\n",
    "            (token,pos) = c_node\n",
    "            if pos == prevPosition:\n",
    "                prevPosition = pos\n",
    "                entity[\"chunk\"].append(token)\n",
    "            elif prevPosition in [\"DT\",\"JJ\"]:\n",
    "                prevPosition = pos\n",
    "                entity[\"pos\"] = pos\n",
    "                entity[\"chunk\"].append(token)\n",
    "            else:\n",
    "                if not len(entity[\"chunk\"]) == 0:\n",
    "                    chunks.append((entity[\"pos\"],\" \".join(entity[\"chunk\"])))\n",
    "                    entity = {\"pos\":pos,\"chunk\":[token]}\n",
    "                    prevPosition = pos\n",
    "        if not len(entity[\"chunk\"]) == 0:\n",
    "            chunks.append((entity[\"pos\"],\" \".join(entity[\"chunk\"])))\n",
    "        return chunks\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully added 110 files.\n"
     ]
    }
   ],
   "source": [
    "#STEP 1 compute file list\n",
    "import os\n",
    "dirName=\"/wiki-pages-text-test/\"\n",
    "\n",
    "# r=root, d=directories, f = files\n",
    "files=[]\n",
    "for r, d, f in os.walk(os.getcwd()+dirName):\n",
    "    for file in f:\n",
    "        if '.txt' in file:\n",
    "            files.append(os.path.join(r, file))\n",
    "print(\"Successfully added\",len(files),\"files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page content Extracted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/ola/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# STEP 2 computeTFIDF for corpus\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "extraStopWords=[\"@\",\"^\",\"\\\"\",\"(\",\")\",\"*\",\"_\",\"\\\\\",\":\",\";\",\"`\",\"!\",\"-RRB-\",\"-LRB-\"]\n",
    "\n",
    "for stopW in extraStopWords:\n",
    "    stopWords.add(stopW)\n",
    "\n",
    "#STEP 2a Tokenise the text\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = []\n",
    "    for item in tokens:\n",
    "        stems.append(PorterStemmer().stem(item))\n",
    "    return stems\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(norm=None, ngram_range=(1,2),stop_words=stopWords,lowercase=True,tokenizer=tokenize)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#STEP 2b Extract Page content   \n",
    "pageList={}\n",
    "pageContent=[]\n",
    "for file in files:\n",
    "    f=open(file,\"r\",encoding=\"utf8\")\n",
    "    for line in f:\n",
    "        line= line.split(' ',2)\n",
    "        pageID=line[0]\n",
    "        sentenceID=line[1]\n",
    "        content=line[2]\n",
    "        pageList[pageID,sentenceID]=content\n",
    "        pageContent.append(content)\n",
    "    f.close()\n",
    "       \n",
    "print(\"Page content Extracted\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus lines merged\n"
     ]
    }
   ],
   "source": [
    "#STEP 2c process the lines in the pages to merge them together\n",
    "prevPageID=\"\"\n",
    "contentList=[]\n",
    "pageIdList=[]\n",
    "newContent=\"\"\n",
    "for (pageID,sentenceID),content in pageList.items():\n",
    "    if prevPageID!=pageID:\n",
    "        prevPageID=pageID\n",
    "        pageIdList.append(pageID)\n",
    "        contentList.append(newContent)\n",
    "        newContent=\"\"\n",
    "        newContent=newContent+\" \"+content\n",
    "    else:\n",
    "        newContent=newContent+\" \"+content\n",
    "\n",
    "\n",
    "\n",
    "print(\"Corpus lines merged\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/ola/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19536 tfidf computed\n"
     ]
    }
   ],
   "source": [
    "#STEP 2d computeTFIDF Matrix\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(contentList)\n",
    "print(len(tfidf_vectorizer.get_feature_names()),\"tfidf computed\")\n",
    "#print(tfidf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extracted\n"
     ]
    }
   ],
   "source": [
    "#STEP 2e get corpus features\n",
    "features=tfidf_vectorizer.get_feature_names()\n",
    "print(\"Feature extracted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 2f PROCESS THE CLAIMS \n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk import pos_tag,ne_chunk\n",
    "import json\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import wordnet\n",
    "from collections import Counter \n",
    "\n",
    "with open('devset.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "claims=[]\n",
    "processedC=ProcessedClaim()\n",
    "processedClaims={}\n",
    "for linenum in data:\n",
    "    claim=data[linenum][\"claim\"]\n",
    "    claims.append((linenum,claim))\n",
    "    #process the claim. Add negation if possible and rephrase it to add antonyms if possible.\n",
    "    processedClaims[claim]=claim\n",
    "    posTagsForClaim=processedC.createPosTags(claim,1)\n",
    "    tempPosTagsForClaim=posTagsForClaim[0]\n",
    "    for i in range(len(tempPosTagsForClaim)):\n",
    "        prev_term = tempPosTagsForClaim[i][1]\n",
    "        if(i>0):\n",
    "            prev_term = tempPosTagsForClaim[i-1][1]\n",
    "            prev_posTag=tempPosTagsForClaim[i-1][0]\n",
    "        term=tempPosTagsForClaim[i][1]\n",
    "        posTag=tempPosTagsForClaim[i][0]\n",
    "        if(posTag in [\"JJ\",\"JJR\",\"JJS\"]):\n",
    "            new_term=processedC.getAntonyms(term)\n",
    "            processedClaims[claim]=claim.replace(term, new_term)\n",
    "        if(posTag in [\"VB\",\"VBD\",\"VBG\",\"VBN\",\"VBP\",\"VBZ\"]):\n",
    "            new_term=processedC.getNegation(prev_term,term)\n",
    "            processedClaims[claim]=claim.replace(term, new_term)\n",
    "\n",
    "print(len(processedClaims),\"processed claims generated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/ola/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/ola/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /Users/ola/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0 ['List_of_San_Francisco_49ers_seasons', 1]\n",
      "[['List_of_San_Francisco_49ers_seasons', 1]]\n",
      "[['List_of_San_Francisco_49ers_seasons', 1]]\n",
      "2\n",
      "0 ['Tilda', 12]\n",
      "[['Tilda', 12]]\n",
      "[['Tilda', 12]]\n",
      "3\n",
      "0 ['Soul_Food_-LRB-film-RRB-', 0]\n",
      "[['Soul_Food_-LRB-film-RRB-', 0]]\n",
      "1 ['Soul_Food_-LRB-film-RRB-', 5]\n",
      "[['Soul_Food_-LRB-film-RRB-', 0], ['Soul_Food_-LRB-film-RRB-', 5]]\n",
      "[['Soul_Food_-LRB-film-RRB-', 0], ['Soul_Food_-LRB-film-RRB-', 5]]\n",
      "4\n",
      "0 ['Bill_Nershi', 0]\n",
      "[['Bill_Nershi', 0]]\n",
      "[['Bill_Nershi', 0]]\n",
      "5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-86c9f55ccf90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpageID\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtopKRanks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mtempPageList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpageID\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msentenceID\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m             \u001b[0mpageCosSimilarity\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpageID\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msentenceID\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcosineSim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-49-86c9f55ccf90>\u001b[0m in \u001b[0;36mcosineSim\u001b[0;34m(query, pageContent)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcosineSim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpageContent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mtfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpageContent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1601\u001b[0m         \"\"\"\n\u001b[1;32m   1602\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1603\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1604\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1605\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m-> 1032\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m   1033\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    932\u001b[0m             \u001b[0mvocabulary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_factory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__len__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 934\u001b[0;31m         \u001b[0manalyze\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_analyzer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    935\u001b[0m         \u001b[0mj_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m         \u001b[0mindptr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mbuild_analyzer\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mtokenize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             self._check_stop_words_consistency(stop_words, preprocess,\n\u001b[0;32m--> 326\u001b[0;31m                                                tokenize)\n\u001b[0m\u001b[1;32m    327\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[1;32m    328\u001b[0m                 tokenize(preprocess(self.decode(doc))), stop_words)\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_check_stop_words_consistency\u001b[0;34m(self, stop_words, preprocess, tokenize)\u001b[0m\n\u001b[1;32m    288\u001b[0m             \u001b[0minconsistent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m                 \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-49-86c9f55ccf90>\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mremove_punctuation_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mstemTokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremove_punctuation_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcosineSim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpageContent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     return [\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m     ]\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     return [\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m     ]\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/tokenize/treebank.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_parentheses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_str\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubstitution\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTARTING_QUOTES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubstitution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubstitution\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPUNCTUATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/re.py\u001b[0m in \u001b[0;36m_subx\u001b[0;34m(pattern, template)\u001b[0m\n\u001b[1;32m    305\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msre_parse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_template\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_subx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemplate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m     \u001b[0;31m# internal: Pattern.sub/subn implementation helper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0mtemplate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_compile_repl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#STEP 3 GET TOP K PAGES FOR THE QUERY\n",
    "import json\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from nltk.tree import Tree\n",
    "from nltk import ne_chunk\n",
    "from nltk import pos_tag\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import wordnet\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def createNERTags1(paragraph):\n",
    "    doc = nlp(paragraph)\n",
    "    cleaned = [y for y in doc if not y.is_stop and y.pos_!=\"PUNCT\"  ]\n",
    "    raw=[(x.lemma_,x.pos_) for x in cleaned]\n",
    "    return raw\n",
    "\n",
    "def matchNERTags1(qTags,pTags):\n",
    "    match=[]\n",
    "    matchCount=0\n",
    "    for (qterm,qtag) in qTags:\n",
    "       for (pterm,ptag) in pTags:\n",
    "            if qtag==ptag :\n",
    "                if qterm==pterm:\n",
    "                    if qtag not in match: \n",
    "                        match.append(qtag)\n",
    "                        matchCount=matchCount+1\n",
    "                        break\n",
    "    if(matchCount==len(qTags)):\n",
    "        label=\"SUPPORTS\"\n",
    "    else:\n",
    "        label=\"REFUTES\"\n",
    "    return label\n",
    "#STEP 3a Find the top k page ranks\n",
    "\n",
    "\n",
    "\n",
    "#STEP 3b Get the best matching page using cosine sim\n",
    "import string\n",
    "def stemTokens(tokens):\n",
    "    return [PorterStemmer().stem(item) for item in tokens]\n",
    "def normalize(text):\n",
    "    remove_punctuation_map = dict((ord(char), None) for char in string.punctuation)\n",
    "    return stemTokens(nltk.word_tokenize(text.lower().translate(remove_punctuation_map)))\n",
    "def cosineSim(query, pageContent):\n",
    "    vectorizer = TfidfVectorizer(tokenizer=normalize, stop_words='english')\n",
    "    tfidf = vectorizer.fit_transform([query, pageContent])\n",
    "    return ((tfidf * tfidf.T).A)[0,1]\n",
    "\n",
    "\n",
    "def createJSON(linenum,claim,label,evidencearray,outer_dict):\n",
    "    inner_dict={}\n",
    "    inner_dict[\"claim\"]=claim\n",
    "    inner_dict[\"label\"]=label\n",
    "    inner_dict[\"evidence\"]=evidencearray\n",
    "    outer_dict[linenum]=inner_dict\n",
    "    with open('predicted.json', 'w',encoding=\"utf8\") as outfile:  \n",
    "        json.dump(outer_dict, outfile)\n",
    "    outfile.close()\n",
    "    #print(linenum,inner_dict)\n",
    "    return outer_dict\n",
    "\n",
    "\n",
    "def queryTokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = []\n",
    "    for item in tokens:\n",
    "        if item not in stems and item not in stopWords:\n",
    "            stems.append(PorterStemmer().stem(item))\n",
    "    return stems\n",
    "\n",
    "\n",
    "\n",
    "#STEP 3b Get the best matching sentence using ngram sim\n",
    "def nGramSimilarity(claim, sentence,nGram):\n",
    "    ps = PorterStemmer()\n",
    "   \n",
    "    getNGram = lambda tokens,n:[ \" \".join([tokens[index+i] for i in range(0,n)]) for index in range(0,len(tokens)-n+1)]\n",
    "    qToken = queryTokenize(claim)\n",
    "    sToken = queryTokenize(sentence)\n",
    "\n",
    "    if(len(qToken) > nGram):\n",
    "        q3gram = set(getNGram(qToken,nGram))\n",
    "        s3gram = set(getNGram(sToken,nGram))\n",
    "        #print (q3gram)\n",
    "        #print (s3gram)\n",
    "        if(len(s3gram) < nGram):\n",
    "            return 0\n",
    "        qLen = len(q3gram)\n",
    "        sLen = len(s3gram)\n",
    "        sim = len(q3gram.intersection(s3gram)) / len(q3gram.union(s3gram))\n",
    "        #print(sim)\n",
    "        return sim\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "#STEP 3c EXTRACT THE ENTITIES\n",
    "\n",
    "def extractEntities(posTagContent,pages):\n",
    "    entities={}\n",
    "    for i in range(0,len(posTagContent)):\n",
    "        for posTag,term in posTagContent[i]:\n",
    "            if(posTag=='NNP'):\n",
    "                entities[term]=pages[mostSimilarPageID].splitlines()[i]\n",
    "    return entities\n",
    "\n",
    "with open('devset.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "outer_dict={}\n",
    "claimCounter=0\n",
    "for linenum in data:\n",
    "    claim=data[linenum][\"claim\"]\n",
    "    evidence=[]\n",
    "    evidencearray=[]\n",
    "    label=\"\"\n",
    "    query=data[linenum][\"claim\"]\n",
    "    claimCounter=claimCounter+1\n",
    "    print(claimCounter)\n",
    "    #STEP 3a tokenise the query\n",
    "    queryUniGram=queryTokenize(query)\n",
    "    #print(\"query unigram created\")\n",
    "    #STEP 3b caculate ngram for queryTokens\n",
    "    queryTokens=[]\n",
    "    getNGram = lambda tokens,n:[ \" \".join([tokens[index+i] for i in range(0,n)]) for index in range(0,len(tokens)-n+1)]\n",
    "    queryNGram = set(getNGram(queryUniGram,2))\n",
    "    \n",
    "    #print(\"query ngram created\")\n",
    "    queryTokens = list(set(queryUniGram) | set(queryNGram))\n",
    "    #print(\"final query token created\")\n",
    "    temptopKRanks=[]\n",
    "    topKRanks=set()\n",
    "    for term in queryTokens:\n",
    "        if term in features:\n",
    "            indexOfQueryTerm=features.index(term)\n",
    "            temptopKRanks=tfidf_matrix[:,indexOfQueryTerm].toarray()\n",
    "            # Find index of maximum value from 2D numpy array\n",
    "            maxV = np.where(temptopKRanks == np.amax(temptopKRanks))\n",
    "            listOfCordinates = list(zip(maxV[0], maxV[1]))\n",
    "            topKRanks.add(pageIdList[maxV[0][0]-1])\n",
    "            #print(\"max done\")\n",
    "            #print (term,pageIdList[maxI-1])\n",
    "    \n",
    "    pageCosSimilarity={}\n",
    "    sentenceNGramSim={}\n",
    "    tempPageList={}\n",
    "    for (pageID,sentenceID),content in pageList.items():\n",
    "        if pageID in topKRanks:\n",
    "            tempPageList[(pageID,sentenceID)]=content\n",
    "            pageCosSimilarity[pageID,sentenceID]=cosineSim(query,content)\n",
    "    \n",
    "\n",
    "    mostSimilarPageID=max(pageCosSimilarity, key=pageCosSimilarity.get)\n",
    "  \n",
    "    for (pageID,sentenceID),content in tempPageList.items():\n",
    "        if pageID==mostSimilarPageID[0]:\n",
    "            sentenceNGramSim[(pageID,sentenceID)]=nGramSimilarity(query,content,2)\n",
    "    avg=0.0      \n",
    "    for (pageID,sentenceID),sim in sentenceNGramSim.items():\n",
    "        avg=avg+sim\n",
    "    avg=avg/len(sentenceNGramSim)\n",
    "    sentenceNGramSim=sorted(sentenceNGramSim.items(), key=lambda kv: kv[1],reverse=True)\n",
    "\n",
    "    \n",
    "    \n",
    "    if(sentenceNGramSim[0][1]<avg):\n",
    "        label=\"NOT ENOUGH INFO\"\n",
    "        outer_dict=createJSON(linenum,claim,label,evidencearray,outer_dict)\n",
    "        continue\n",
    "        \n",
    "    \n",
    "    qNERTags=createNERTags1(query)\n",
    "    pNERTags=createNERTags1(tempPageList[sentenceNGramSim[0][0]])\n",
    "    \n",
    "    #label=matchNERTags(qNERTags,pNERTags)\n",
    "    label=matchNERTags1(qNERTags,pNERTags)\n",
    "    for i in range(len(sentenceNGramSim)):\n",
    "        #print(sentenceNGramSim[i][1],avg)\n",
    "        if sentenceNGramSim[i][1]>=avg:\n",
    "            evidence=[]\n",
    "            evidence.append(sentenceNGramSim[i][0][0])\n",
    "            evidence.append(int(sentenceNGramSim[i][0][1]))\n",
    "            evidencearray.append(evidence)\n",
    "    outer_dict=createJSON(linenum,claim,label,evidencearray,outer_dict)\n",
    "print(outer_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 3 GET TOP K PAGES FOR THE QUERY\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "#STEP 3a Find the top k page ranks\n",
    "\n",
    "#STEP 4b get corpus features\n",
    "features=tfidf_vectorizer.get_feature_names()\n",
    "print(\"Feature extracted\")\n",
    "#STEP 5 Get the best matching page using cosine sim\n",
    "import string\n",
    "def stemTokens(tokens):\n",
    "    return [PorterStemmer().stem(item) for item in tokens]\n",
    "def normalize(text):\n",
    "    remove_punctuation_map = dict((ord(char), None) for char in string.punctuation)\n",
    "    return stemTokens(nltk.word_tokenize(text.lower().translate(remove_punctuation_map)))\n",
    "def cosineSim(query, pageContent):\n",
    "    vectorizer = TfidfVectorizer(tokenizer=normalize, stop_words='english')\n",
    "    tfidf = vectorizer.fit_transform([query, pageContent])\n",
    "    return ((tfidf * tfidf.T).A)[0,1]\n",
    "\n",
    "\n",
    "\n",
    "def queryTokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = []\n",
    "    for item in tokens:\n",
    "        if item not in stems and item not in stopWords:\n",
    "            stems.append(PorterStemmer().stem(item))\n",
    "    return stems\n",
    "\n",
    "with open('devset.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "for linenum in data:\n",
    "    query=data[linenum][\"claim\"]\n",
    "    print()\n",
    "    print(query)\n",
    "    #STEP 3a tokenise the query\n",
    "    queryUniGram=queryTokenize(query)\n",
    "    #print(\"query unigram created\")\n",
    "    #STEP 3b caculate ngram for queryTokens\n",
    "    queryTokens=[]\n",
    "    getNGram = lambda tokens,n:[ \" \".join([tokens[index+i] for i in range(0,n)]) for index in range(0,len(tokens)-n+1)]\n",
    "    queryNGram = set(getNGram(queryUniGram,2))\n",
    "    print(queryNGram)\n",
    "    #print(\"query ngram created\")\n",
    "    queryTokens = list(set(queryUniGram) | set(queryNGram))\n",
    "    #print(\"final query token created\")\n",
    "    temptopKRanks=[]\n",
    "    topKRanks=set()\n",
    "    for term in queryTokens:\n",
    "        if term in features:\n",
    "            indexOfQueryTerm=features.index(term)\n",
    "            temptopKRanks=tfidf_matrix[:,indexOfQueryTerm].toarray()\n",
    "            i=0\n",
    "            maxV=0\n",
    "            for a in temptopKRanks:\n",
    "                if a[0]>maxV:\n",
    "                    maxV=a[0]\n",
    "                    maxI=i\n",
    "                i=i+1\n",
    "            topKRanks.add(pageIdList[maxI-1])\n",
    "            #print (term,pageIdList[maxI-1])\n",
    "    print(topKRanks)\n",
    "    pageCosSimilarity={}\n",
    "    for (pageID,sentenceID),content in pageList.items():\n",
    "        if pageID in topKRanks:\n",
    "            pageCosSimilarity[pageID,sentenceID]=cosineSim(query,content)\n",
    "\n",
    "    mostSimilarPageID=max(pageCosSimilarity, key=pageCosSimilarity.get)\n",
    "    print (\"Most Similar Page is \",mostSimilarPageID,)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
